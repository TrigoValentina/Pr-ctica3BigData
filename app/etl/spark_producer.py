import os
import json
import time
import logging
from kafka import KafkaProducer
import pandas as pd
from datetime import datetime, timezone
from pathlib import Path
import traceback

# ---------------- CONFIGURACI√ìN DE LOG DETALLADA ---------------- 
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)

logger.info("=" * 80)
logger.info("üöÄ INICIANDO PRODUCTOR KAFKA - INGESTA DE DATOS DE SENSORES (DOCKER)")
logger.info("=" * 80)

# ---------------- PRODUCTOR KAFKA ---------------- 
KAFKA_BROKER = 'kafka:9092'  # Para uso dentro de Docker
KAFKA_TOPIC = 'datos_sensores'

logger.info(f"üì° Intentando conectar a Kafka en: {KAFKA_BROKER}")
logger.info(f"üì¨ T√≥pico destino: {KAFKA_TOPIC}")
logger.info(f"üê≥ Modo: Docker (contenedor)")

try:
    producer = KafkaProducer(
        bootstrap_servers=[KAFKA_BROKER],
        value_serializer=lambda v: json.dumps(v, ensure_ascii=False, default=str).encode('utf-8'),
        api_version=(0, 10, 1)
    )
    logger.info("‚úÖ Productor Kafka creado exitosamente")
    logger.info(f"   - Broker: {KAFKA_BROKER}")
    logger.info(f"   - T√≥pico: {KAFKA_TOPIC}")
except Exception as e:
    logger.error(f"‚ùå Error al crear productor Kafka: {e}")
    logger.error(traceback.format_exc())
    raise

# ---------------- CARPETA DE DATOS ---------------- 
logger.info("-" * 80)
logger.info("üìÇ CONFIGURANDO CARPETA DE DATOS")
logger.info("-" * 80)

# En Docker, los datos est√°n montados en /opt/spark/data
data_folder = Path("/opt/spark/data")
logger.info(f"üìÅ Carpeta de datos (Docker): {data_folder}")
logger.info(f"üìÅ ¬øExiste la carpeta?: {data_folder.exists()}")

archivos_esperados = [
    "EM310-UDL-915M soterrados nov 2024.csv",
    "EM500-CO2-915M nov 2024.csv",
    "WS302-915M SONIDO NOV 2024.csv"
]

logger.info(f"üìã Archivos CSV esperados ({len(archivos_esperados)}):")
for archivo in archivos_esperados:
    logger.info(f"   - {archivo}")

# ---------------- COLUMNAS POR CSV ---------------- 
logger.info("-" * 80)
logger.info("üìä CONFIGURACI√ìN DE COLUMNAS POR ARCHIVO CSV")
logger.info("-" * 80)

columnas_por_csv = {
    "EM310-UDL-915M soterrados nov 2024.csv": [
        "time",
        "deviceInfo.deviceName",
        "deviceInfo.tags.Address",
        "deviceInfo.tags.Location",
        "object.distance",
        "object.status"
    ],
    "EM500-CO2-915M nov 2024.csv": [
        "time",
        "deviceInfo.deviceName",
        "deviceInfo.tags.Address",
        "deviceInfo.tags.Location",
        "object.co2_status",
        "object.co2",
        "object.temperature_message",
        "object.pressure_message",
        "object.pressure",
        "object.co2_message",
        "object.pressure_status",
        "object.humidity_status",
        "object.temperature",
        "object.humidity",
        "object.humidity_message",
        "object.temperature_status"
    ],
    "WS302-915M SONIDO NOV 2024.csv": [
        "time",
        "deviceInfo.tenantName",
        "deviceInfo.tags.Address",
        "deviceInfo.tags.Location",
        "object.LAeq",
        "object.LAI",
        "object.LAImax",
        "object.status"
    ]
}

for archivo, columnas in columnas_por_csv.items():
    logger.info(f"üìÑ {archivo}:")
    logger.info(f"   Total columnas: {len(columnas)}")
    for col in columnas:
        logger.info(f"      - {col}")

# ---------------- PROCESAMIENTO Y ENV√çO ---------------- 
logger.info("=" * 80)
logger.info("üîÑ INICIANDO PROCESAMIENTO Y ENV√çO DE DATOS")
logger.info("=" * 80)

total_registros_enviados = 0
total_archivos_procesados = 0

for archivo in archivos_esperados:
    try:
        csv_path = data_folder / archivo
        logger.info("-" * 80)
        logger.info(f"üìÑ PROCESANDO ARCHIVO: {archivo}")
        logger.info(f"   Ruta completa: {csv_path}")
        logger.info("-" * 80)
        
        if not csv_path.exists():
            logger.warning(f"‚ö†Ô∏è Archivo no encontrado: {archivo}")
            logger.warning(f"   Ruta buscada: {csv_path}")
            continue

        columnas_filtradas = columnas_por_csv.get(archivo, [])
        if not columnas_filtradas:
            logger.warning(f"‚ö†Ô∏è No hay configuraci√≥n de columnas para: {archivo}")
            continue

        logger.info(f"üìä Leyendo CSV: {csv_path}")
        logger.info(f"   Columnas configuradas: {len(columnas_filtradas)}")
        
        df = pd.read_csv(csv_path, low_memory=False)
        logger.info(f"‚úÖ CSV le√≠do exitosamente")
        logger.info(f"   Filas originales: {len(df)}")
        logger.info(f"   Columnas en CSV: {len(df.columns)}")
        
        df = df.dropna(how="all")
        total_filas = len(df)
        logger.info(f"   Filas despu√©s de limpiar vac√≠as: {total_filas}")
        
        # Verificar columnas esperadas
        columnas_faltantes = [col for col in columnas_filtradas if col not in df.columns]
        if columnas_faltantes:
            logger.warning(f"‚ö†Ô∏è Columnas faltantes en CSV ({len(columnas_faltantes)}):")
            for col in columnas_faltantes:
                logger.warning(f"      - {col}")
        else:
            logger.info(f"‚úÖ Todas las columnas esperadas est√°n presentes")
        
        columnas_encontradas = [col for col in columnas_filtradas if col in df.columns]
        logger.info(f"üìã Columnas que se procesar√°n: {len(columnas_encontradas)}/{len(columnas_filtradas)}")
        logger.info(f"üì§ Iniciando env√≠o de {total_filas} registros a Kafka...")
        
        registros_enviados_archivo = 0

        for i, row in df.iterrows():
            try:
                data = {}
                logger.debug(f"   Procesando fila {i+1}/{total_filas}")

                for col in columnas_por_csv[archivo]:
                    if col in df.columns:
                        valor = row[col]
                        
                        # Para EM500, incluir campos incluso si est√°n vac√≠os para que el consumer los detecte
                        # Solo omitir si es completamente NaN y no es un campo cr√≠tico de identificaci√≥n
                        if pd.isna(valor):
                            # Si es un campo de object (co2, temperature, etc.), incluir como None
                            if col.startswith("object."):
                                valor = None  # Incluir como None en lugar de omitir
                                logger.debug(f"      ‚ö†Ô∏è Columna '{col}' tiene valor NaN, incluyendo como None")
                            else:
                                logger.debug(f"      ‚ö†Ô∏è Columna '{col}' tiene valor NaN, omitiendo")
                                continue
                        
                        parts = col.split(".")
                        ref = data
                        for p in parts[:-1]:
                            if p not in ref:
                                ref[p] = {}
                            ref = ref[p]
                        ref[parts[-1]] = valor
                        logger.debug(f"      ‚úÖ Agregado: {col} = {valor}")

                # Si no hay "time", usar timestamp actual
                if "time" not in data or pd.isna(data.get("time")):
                    data["time"] = datetime.now(timezone.utc).isoformat()
                    logger.debug(f"      ‚è∞ Timestamp generado: {data['time']}")
                elif isinstance(data.get("time"), pd.Timestamp):
                    data["time"] = data["time"].isoformat()
                    logger.debug(f"      ‚è∞ Timestamp convertido: {data['time']}")

                # Para EM500, asegurar que siempre haya un objeto "object" aunque est√© vac√≠o
                # Esto permite que el consumer detecte el tipo de sensor por el nombre del dispositivo
                if archivo == "EM500-CO2-915M nov 2024.csv" and "object" not in data:
                    data["object"] = {}
                    logger.debug(f"      ‚úÖ Objeto 'object' creado vac√≠o para EM500")

                if not data or ("time" not in data and "deviceInfo" not in data):
                    logger.warning(f"   ‚ö†Ô∏è Fila {i+1} result√≥ vac√≠a, saltando...")
                    continue

                # Enviar a Kafka
                logger.debug(f"   üì® Enviando registro {i+1} a Kafka...")
                future = producer.send(KAFKA_TOPIC, value=data)
                
                try:
                    record_metadata = future.get(timeout=10)
                    logger.info(f"üì® [{archivo}] Registro {i+1}/{total_filas} enviado exitosamente")
                    logger.info(f"      T√≥pico: {record_metadata.topic}")
                    logger.info(f"      Partici√≥n: {record_metadata.partition}")
                    logger.info(f"      Offset: {record_metadata.offset}")
                    logger.debug(f"      Datos: {json.dumps(data, ensure_ascii=False, default=str)}")
                    registros_enviados_archivo += 1
                    total_registros_enviados += 1
                except Exception as e:
                    logger.error(f"   ‚ùå Error al enviar registro {i+1}: {e}")
                    logger.error(f"      Datos: {json.dumps(data, ensure_ascii=False, default=str)}")

                # Reducir sleep para acelerar el procesamiento
                if archivo == "EM500-CO2-915M nov 2024.csv":
                    time.sleep(0.01)  # 0.01 segundos para EM500 (muy r√°pido)
                elif archivo == "WS302-915M SONIDO NOV 2024.csv":
                    time.sleep(0.01)  # 0.01 segundos para WS302 (muy r√°pido)
                else:
                    time.sleep(0.1)  # 0.1 segundos para EM310

            except Exception as e:
                logger.error(f"   ‚ùå Error procesando fila {i+1}: {e}")
                logger.error(traceback.format_exc())
                continue

        logger.info(f"‚úÖ Env√≠o completado para {archivo}")
        logger.info(f"   Registros enviados: {registros_enviados_archivo}/{total_filas}")
        total_archivos_procesados += 1

    except Exception as e:
        logger.error(f"‚ùå Error procesando archivo {archivo}: {e}")
        logger.error(traceback.format_exc())
        continue

# ---------------- FINALIZAR ---------------- 
logger.info("=" * 80)
logger.info("üèÅ FINALIZANDO ENV√çO DE DATOS")
logger.info("=" * 80)

logger.info("üîÑ Flusheando mensajes pendientes en Kafka...")
producer.flush()
logger.info("‚úÖ Flush completado")

logger.info("=" * 80)
logger.info("üìä RESUMEN FINAL")
logger.info("=" * 80)
logger.info(f"‚úÖ Archivos procesados: {total_archivos_procesados}/{len(archivos_esperados)}")
logger.info(f"‚úÖ Total de registros enviados: {total_registros_enviados}")
logger.info(f"‚úÖ T√≥pico Kafka: {KAFKA_TOPIC}")
logger.info(f"‚úÖ Broker: {KAFKA_BROKER}")
logger.info("=" * 80)
logger.info("üéØ Proceso completado exitosamente")
logger.info("=" * 80)
